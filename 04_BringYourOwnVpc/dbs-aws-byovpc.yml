AWSTemplateFormatVersion: 2010-09-09
Description: Deploying a Databricks Workspace in an existing VPC

#-------------------------------------------------------------------------
Parameters:

  ResourceOwner:
    Description: The value of the Owner tag in the created resources
    Type: String

  DBSRootBucket:
    Description: The name of the bucket holding the Databricks workspace data
    Type: String
    AllowedPattern: '(?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)'
    ConstraintDescription: 3 to 63 characters; must contain only lowercase letters, numbers, periods (.), and dashes (-)

  VpcId:
    Description: The Id of the VPC where the Databricks cluster is deployed
    Type: String

  ClusterSubnetId1:
    Description: The Id of the VPC subnet for the Databricks clusters in the first availability zone
    Type: String

  ClusterSubnetId2:
    Description: The Id of the VPC subnet for the Databricks clusters in the second availability zone
    Type: String

  WorkspaceSecurityGroupId:
    Description: The security group id used for the Databricks clusters in the VPC
    Type: String

  DBSPrivateLinkMode:
    Description: The Private Link setup mode for the Databricks workspace
    Type: String
    AllowedValues:
      - "Off"
      - "PublicAccessEnabled"
      - "PublicAccessDisabled"
    Default: "Off"

  RelayInterfaceEndpointId:
    Description: The Id of the VPC Endpoint for the Databricks SCC Relay
    Type: String
    Default: ''

  RestApiInterfaceEndpointId:
    Description: The Id of the VPC Endpoint for the Databricks REST API
    Type: String
    Default: ''

  PrivateLinkSubnetId1:
    Description: The Id of the VPC subnet for the PrivateLink endpoint in the first availability zone
    Type: String
    Default: ""

  PrivateLinkSubnetId2:
    Description: The Id of the VPC subnet for the PrivateLink endpoint in the second availability zone
    Type: String
    Default: ""

  DBSEncryptionKeyArn:
    Description: The ARN of the KMS key used for the encryption/decryption of Databricks notebooks
    Type: String
    Default: ""

  DBSEncryptionKeyAlias:
    Description: The alias of the KMS key used for the encryption/decryption of Databricks notebooks
    Type: String
    Default: ""
    
  DBSCrossAccountRoleArn:
    Description: An existing ARN for the cross-account IAM role used by Databricks to manage resources in the VPC
    Type: String
    Default: ""

  DBSAccountId:
    Description: The Databricks account id
    Type: String

  DBSUsername:
    Description: The Databricks account user name
    Type: String

  DBSPassword:
    Description: The Databricks account password
    Type: String
    NoEcho: true

#-------------------------------------------------------------------------
Conditions:

  CreateCrossAccountRole:
    !Equals [!Ref DBSCrossAccountRoleArn, ""]

  EnablePrivateLink:
    !Not [!Equals [!Ref DBSPrivateLinkMode, "Off"] ]

  PublicAccessForPrivateLink:
    !Equals [!Ref DBSPrivateLinkMode, "PublicAccessEnabled"]

  ExistsFirstSubnetForPrivateLink:
    !Not [!Equals [!Ref PrivateLinkSubnetId1, ""] ]

  ExistsSecondSubnetForPrivateLink:
    !Not [!Equals [!Ref PrivateLinkSubnetId2, ""] ]

  UseEncryptionKey:
    !Not [!Equals [!Ref DBSEncryptionKeyArn, ""] ]
  
  ExistsEncryptionKeyAlias:
    !Not [!Equals [!Ref DBSEncryptionKeyAlias, ""] ]

#-------------------------------------------------------------------------
Rules:

  VpcEndpointsForPrivateLink:
    RuleCondition: !Not [!Equals [!Ref DBSPrivateLinkMode, "Off"] ]
    Assertions:
      - Assert: !And [ !Not [!Equals [!Ref RelayInterfaceEndpointId, ""] ], !Not [!Equals [!Ref RestApiInterfaceEndpointId, ""] ] ]
        AssertDescription: Both VPC endpoint Ids for private link need to be specified


#-------------------------------------------------------------------------
Resources:

  # The databricks addresses
  DatabricksAddresses:
    Type: Custom::DatabricksAddresses
    Properties:
      ServiceToken: !ImportValue dbs-account-api-DatabricksAddressesFn-Arn

  # The cross account role for databricks
  DBSCrossAccountIAMRole:
    Condition: CreateCrossAccountRole
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}-CrossAccountRole
      Description: The cross-account IAM role used by Databricks to manage resources in the VPC
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              "AWS": "414351767826"
            Action:
              - 'sts:AssumeRole'
            Condition:
              "StringEquals":
                "sts:ExternalId": !Ref DBSAccountId
      Policies:
        - PolicyName: !Sub ${AWS::StackName}-CrossAccountRolePolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Sid: NonResourceBasedPermissions
                Effect: Allow
                Action:
                  - ec2:CancelSpotInstanceRequests
                  - ec2:DescribeAvailabilityZones
                  - ec2:DescribeIamInstanceProfileAssociations
                  - ec2:DescribeInstanceStatus
                  - ec2:DescribeInstances
                  - ec2:DescribeInternetGateways
                  - ec2:DescribeNatGateways
                  - ec2:DescribeNetworkAcls
                  - ec2:DescribePrefixLists
                  - ec2:DescribeReservedInstancesOfferings
                  - ec2:DescribeRouteTables
                  - ec2:DescribeSecurityGroups
                  - ec2:DescribeSpotInstanceRequests
                  - ec2:DescribeSpotPriceHistory
                  - ec2:DescribeSubnets
                  - ec2:DescribeVolumes
                  - ec2:DescribeVpcAttribute
                  - ec2:DescribeVpcs
                  - ec2:CreateTags
                  - ec2:DeleteTags
                  - ec2:RequestSpotInstances
                  - ec2:RunInstances
                Resource: '*'
              - Sid: AllowPassRoleForInstanceProfile
                Effect: Allow
                Action: iam:PassRole
                # Resource: !GetAtt DBSInstanceProfileRole.Arn
                Resource: '*'
              - Sid: InstancePoolsSupport
                Effect: Allow
                Action:
                  - ec2:AssociateIamInstanceProfile
                  - ec2:DisassociateIamInstanceProfile
                  - ec2:ReplaceIamInstanceProfileAssociation
                Resource: !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
                Condition:
                  "StringEquals":
                    "ec2:ResourceTag/Vendor": "Databricks"
              - Sid: EC2TerminateInstancesTag
                Effect: Allow
                Action: ec2:TerminateInstances
                Resource: !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
                Condition:
                  "StringEquals":
                    "ec2:ResourceTag/Vendor": "Databricks"
              - Sid: EC2AttachDetachVolumeTag
                Effect: Allow
                Action:
                  - ec2:AttachVolume
                  - ec2:DetachVolume
                Resource:
                  - !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:instance/*
                  - !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:volume/*
                Condition:
                  "StringEquals":
                    "ec2:ResourceTag/Vendor": "Databricks"
              - Sid: EC2CreateVolumeByTag
                Effect: Allow
                Action: ec2:CreateVolume
                Resource: !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:volume/*
                Condition:
                  "StringEquals":
                    "ec2:ResourceTag/Vendor": "Databricks"
              - Sid: EC2DeleteVolumeByTag
                Effect: Allow
                Action: ec2:DeleteVolume
                Resource: !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:volume/*
                Condition:
                  "StringEquals":
                    "ec2:ResourceTag/Vendor": "Databricks"
              - Sid: AllowCreateServiceLinkedRole
                Effect: Allow
                Action:
                  - iam:CreateServiceLinkedRole
                  - iam:PutRolePolicy
                Resource: arn:aws:iam::*:role/aws-service-role/spot.amazonaws.com/AWSServiceRoleForEC2Spot
                Condition:
                  "StringLike":
                    "iam:AWSServiceName": spot.amazonaws.com
              - Sid: VpcNonresourceSpecificActions
                Effect: Allow
                Action:
                  - ec2:AuthorizeSecurityGroupEgress
                  - ec2:AuthorizeSecurityGroupIngress
                  - ec2:RevokeSecurityGroupEgress
                  - ec2:RevokeSecurityGroupIngress
                Resource: !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:security-group/${WorkspaceSecurityGroupId}
                Condition:
                  "StringEquals":
                    "ec2:vpc": !Sub arn:${AWS::Partition}:ec2:${AWS::Region}:${AWS::AccountId}:vpc/${VpcId}
      Tags:
        - Key: Owner
          Value: !Ref ResourceOwner

  #------ The Databricks workspace related resources
  DBSCredentialsConfiguration:
    Type: Custom::DBSCredentialsConfiguration
    Properties:
      ServiceToken: !ImportValue dbs-account-api-CredentialsConfigurationFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      CredentialsName: !Sub ${AWS::StackName}_credentials
      CrossAccountRoleArn: !If [CreateCrossAccountRole, !GetAtt DBSCrossAccountIAMRole.Arn, !Ref DBSCrossAccountRoleArn]

  DBSStorageConfiguration:
    Type: Custom::DBSStorageConfiguration
    Properties:
      ServiceToken: !ImportValue dbs-account-api-StorageConfigurationFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      StorageName: !Sub ${AWS::StackName}_storage
      RootBucket: !Ref DBSRootBucket

  DBSManagedKeysConfiguration:
    Condition: UseEncryptionKey
    Type: Custom::DBSManagedKeysConfiguration
    Properties:
      ServiceToken: !ImportValue dbs-account-api-ManagedKeysConfigurationFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      KeyArn: !Ref DBSEncryptionKeyArn
      KeyAlias: !If [ ExistsEncryptionKeyAlias, !Sub "alias/${DBSEncryptionKeyAlias}", !Ref AWS::NoValue ]
      UseCases:
        - MANAGED_SERVICES

  WorkspaceVpcEnpoint:
    Condition: EnablePrivateLink
    Type: Custom::WorkspaceVpcEnpoint
    Properties:
      ServiceToken: !ImportValue dbs-account-api-VpcEnpointFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      EndpointName: !Sub ${AWS::StackName}_workspaceVpcEndpoint
      VpcEndpointId: !Ref RestApiInterfaceEndpointId
      VpcId: !Ref VpcId
      Subnets:
        - !If
          - ExistsFirstSubnetForPrivateLink
          - !Ref PrivateLinkSubnetId1
          - !Ref ClusterSubnetId1
        - !If
          - ExistsSecondSubnetForPrivateLink
          - !Ref PrivateLinkSubnetId2
          - !Ref ClusterSubnetId2
      SecurityGroups:
        - !Ref WorkspaceSecurityGroupId

  BackendVpcEnpoint:
    Condition: EnablePrivateLink
    Type: Custom::BackendVpcEnpoint
    Properties:
      ServiceToken: !ImportValue dbs-account-api-VpcEnpointFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      EndpointName: !Sub ${AWS::StackName}_backendVpcEndpoint
      VpcEndpointId: !Ref RelayInterfaceEndpointId
      VpcId: !Ref VpcId
      Subnets:
        - !If
          - ExistsFirstSubnetForPrivateLink
          - !Ref PrivateLinkSubnetId1
          - !Ref ClusterSubnetId1
        - !If
          - ExistsSecondSubnetForPrivateLink
          - !Ref PrivateLinkSubnetId2
          - !Ref ClusterSubnetId2
      SecurityGroups:
        - !Ref WorkspaceSecurityGroupId

  DBSNetworkConfiguration:
    Type: Custom::DBSNetworkConfiguration
    Properties:
      ServiceToken: !ImportValue dbs-account-api-NetworkConfigurationFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      NetworkName: !Sub ${AWS::StackName}_network
      VpcId: !Ref VpcId
      Subnets:
        - !Ref ClusterSubnetId1
        - !Ref ClusterSubnetId2
      SecurityGroups:
        - !Ref WorkspaceSecurityGroupId
      VpcEndpoints: !If
        - EnablePrivateLink
        - RestApiEndpointId: !Ref WorkspaceVpcEnpoint
          DataplaneRelayEndpointId: !Ref BackendVpcEnpoint
        - !Ref AWS::NoValue

  PrivateAccessConfiguration:
    Condition: EnablePrivateLink
    Type: Custom::PrivateAccessConfiguration
    Properties:
      ServiceToken: !ImportValue dbs-account-api-PrivateAccessConfigurationFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      PrivateAccessSettingsName:  !Sub ${AWS::StackName}_privateAccessSettings
      PublicAccessEnabled: !If [ PublicAccessForPrivateLink, true, false ]
      AllowedVpcEndpoints:
        - !Ref WorkspaceVpcEnpoint

  DatabricksWorkspace:
    Type: Custom::DatabricksWorkspace
    Properties:
      ServiceToken: !ImportValue dbs-account-api-WorkspaceFn-Arn
      DatabricksAccountId: !Ref DBSAccountId
      Username: !Ref DBSUsername
      Password: !Ref DBSPassword
      WorkspaceName: !Sub ${AWS::StackName}_workspace
      CredentialsId: !Ref DBSCredentialsConfiguration
      StorageId: !Ref DBSStorageConfiguration
      NetworkId: !Ref DBSNetworkConfiguration
      ManagedServicesKeyId: !If [ UseEncryptionKey, !Ref DBSManagedKeysConfiguration, !Ref AWS::NoValue ]
      PrivateAccessId: !If [ EnablePrivateLink, !Ref PrivateAccessConfiguration, !Ref AWS::NoValue ]


#-------------------------------------------------------------------------
Outputs:

  DatabricksWorkspaceId:
    Description: The Id of the Databricks workspace
    Value: !Ref DatabricksWorkspace

  DatabricksDeploymentName:
    Description: The name of the Databricks workspace deployment
    Value: !GetAtt DatabricksWorkspace.DeploymentName